{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Stack Overflow Data in Python\n",
    "\n",
    "In this notebook, we visualize posts on Stack Overflow from the first 14 days of September 2017. The data was compiled from searches on the [Stack Exchange Data Explorer](https://data.stackexchange.com/stackoverflow/query/new). Location information was added using the [Google Maps API](https://developers.google.com/maps/).\n",
    "\n",
    "If you don't want to download information as we go through this tutorial, download the following files locally:\n",
    "- [stack overflow posts](https://docs.google.com/spreadsheets/d/1Uz3F_jCNHTo84k1jVQGGa_n_ZQpBUNKyI0ij6Bc3AlM/export?format=csv)\n",
    "- [stop words](https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt)\n",
    "\n",
    "and run the following code snippet in python\n",
    "```\n",
    "import bokeh.sampledata as sampledata\n",
    "sampledata.download()\n",
    "```\n",
    "\n",
    "<div id=\"contents\"></div>\n",
    "## Table of Contents\n",
    "1. [Load the Data](#load)\n",
    "1. [Visualize Completeness](#completeness)\n",
    "1. [Explore Text](#explore)\n",
    "1. [Visualize Time](#time)\n",
    "1. [Plot Connections](#network)\n",
    "1. [Plot Place](#place)\n",
    "1. [Conclusion](#conclusion)\n",
    "\n",
    "Make sure to go through the first two sections (data and completeness) first. The other sections can be done out of order.\n",
    "\n",
    "## Load Libraries\n",
    "This cell contains all the libraries which are necessary for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "# A nice library for reading in csv data\n",
    "import pandas as pd\n",
    "# A library which most visualization libraries in Python are built on.\n",
    "# We will start by using it to make some plots with pandas\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# A library for doing math\n",
    "import numpy as np\n",
    "# A library for turning unicode fields into ASCII fields\n",
    "import unicodedata\n",
    "# a regex library\n",
    "import re\n",
    "# a class which makes counting the number of times something occurs in a list easier\n",
    "from collections import Counter\n",
    "\n",
    "# some functions for displaying html in a notebook\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# A library to visualize holes in a dataset\n",
    "import missingno as msno\n",
    "\n",
    "# a fancy library for numerical plots\n",
    "import seaborn as sns\n",
    "\n",
    "# Libraries for Word Trees\n",
    "# lets us use graphviz in python\n",
    "from pydotplus import graphviz\n",
    "# to display the final Image\n",
    "from IPython.display import Image\n",
    "\n",
    "# Libraries interactive charts\n",
    "from bokeh.io import output_notebook\n",
    "# display interactive charts inline\n",
    "output_notebook()\n",
    "from bokeh.palettes import Viridis6 as palette\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool, ColorBar, LinearColorMapper, FixedTicker, ColumnDataSource, LogColorMapper\n",
    "# to make patches into glyphs and treat counties and states differently\n",
    "from bokeh.models.glyphs import Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following import is a pair of shape files we use for making maps. If you haven't downloaded these shape files previously, you will need to run `bokeh.sampledata.download()`, hence the `try` statement.\n",
    "\n",
    "If you don't want to download these files, you will miss out on part of the mapping section at the end of this notebook, but otherwise the notebook will be unaffected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # shape files for US counties\n",
    "    from bokeh.sampledata.us_counties import data as counties\n",
    "    # shape files for US states\n",
    "    from bokeh.sampledata import us_states as us_states_data\n",
    "except RuntimeError as e:\n",
    "    # comment these two lines out if you have previously run them\n",
    "    import bokeh.sampledata as sampledata\n",
    "    sampledata.download()\n",
    "    \n",
    "    # shape files for US counties\n",
    "    from bokeh.sampledata.us_counties import data as counties\n",
    "    # shape files for US states\n",
    "    from bokeh.sampledata import us_states as us_states_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"load\"></div>\n",
    "## Load the Data\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "Our data is Stack Overflow posts from the first 14 days of September 2017. The data was compiled from searches on the [Stack Exchange Data Explorer](https://data.stackexchange.com/stackoverflow/query/new). It is being storred on [Google Drive](https://docs.google.com/spreadsheets/d/1Uz3F_jCNHTo84k1jVQGGa_n_ZQpBUNKyI0ij6Bc3AlM/edit?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://docs.google.com/spreadsheets/d/1Uz3F_jCNHTo84k1jVQGGa_n_ZQpBUNKyI0ij6Bc3AlM/export?format=csv'\n",
    "# url = '<path to local file>' # uncomment this line and add the path if you have downloaded the data locally\n",
    "\n",
    "# load the data\n",
    "posts = pd.read_csv(url)\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns\n",
    "What do all these columns mean?\n",
    "1. PostId = the id in Stack Overflow's database of this post\n",
    "1. Score = the score given to the post by people voting up and down on it\n",
    "1. PostType = What type of post is this?\n",
    "1. CreationData = When was this post posted?\n",
    "1. Title = The text in the title of the post\n",
    "1. UserId = The id of the user who posted in the Stack Overflow database\n",
    "1. Reputation = The reputaiton of the user who posted\n",
    "1. Location = The location the user put down as their home on their profile\n",
    "1. Tags = Tags which are associated with this post\n",
    "1. QuestionId = The question this post is linked to\n",
    "\n",
    "Let us convert CreationDate to a datetime type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['CreationDate'] = pd.to_datetime(posts['CreationDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"completeness\"></div>\n",
    "## Visualizing Completeness\n",
    "*[Table of Contents](#contents)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to know how complete our data is, so let's look at which fields have null values for the answers and questions using [missingno](https://github.com/ResidentMario/missingno).\n",
    "\n",
    "Black indicates that the data is present while white indicates that it is missing. The column on the far right is meant to show a chart of how many variables each data point has. It is a bit hard to see on a dataset this large, but if you change the command to `msno.matrix(posts.sample(100))` it will make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any correlation between when columns are null? Why don't we have a single row which is complete in all 10 columns?\n",
    "\n",
    "Missingno has a lot of ways of visualizing data completeness. One of them is to see the correlation in missing-ness between fields. It does this by generating a heatmap where blue indicates two fields tend to go missing together and red indicates two fields tend to be mutually exclusive. Fields which are always present are excluded from the heatmap, and correlations below 0.1 are grayed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.heatmap(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that posts with title information always have tag information and never have a question id.\n",
    "\n",
    "I wonder if these groups make up different types of posts. Let's investigate which post types we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_type_counts = posts['PostType'].value_counts()\n",
    "post_type_counts.plot(kind='bar', color='DarkBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a way to make this chart interactive so we can see how many \"TagWiki\"s and \"TagWikiExcerpt\"s there are?\n",
    "\n",
    "Let's use [Bokeh](https://bokeh.pydata.org/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS = \"pan,wheel_zoom,reset,hover,save\"\n",
    "\n",
    "p = figure(\n",
    "    title=\"Post Types\",\n",
    "    tools=TOOLS,\n",
    "    x_range=post_type_counts.index.tolist(),\n",
    "#     y_axis_type=\"log\",\n",
    "    plot_height=400\n",
    ")\n",
    "p.vbar(x=post_type_counts.index.values, top=post_type_counts.values, width=0.9)\n",
    "\n",
    "hover = p.select_one(HoverTool)\n",
    "hover.point_policy = \"follow_mouse\"\n",
    "hover.tooltips = [(\"Number of Posts\", \"@top\")]\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most posts are either questions or answers. These types of posts serve very different purposes, so let's seperate them out and see how complete each is.\n",
    "\n",
    "This time we'll try a different method of visualizing missing data in which we count up how often each attribute is not missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = posts[posts['PostType'] == 'Question']\n",
    "answers = posts[posts['PostType'] == 'Answer']\n",
    "\n",
    "print(\"Questions\")\n",
    "msno.bar(questions)\n",
    "print(\"Answers\")\n",
    "msno.bar(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that UserId is missing for 1,171 questions and 785 answers. This is only .4% of the data overall, but it seems strange that a post can exist without a user to make it.\n",
    "\n",
    "Usually, when you fing out that you having missing data you want to know why and what is going on with those points. Fortunately Pandas makes this very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[posts['UserId'].isnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to investigate these posts further, we can see posts and answers with null `UserId`s using Jupyter's HTML capabilities. The following code creates links we can click on to see each post on Stack Overflow. Compare the user descriptions for the linked posts to the user descriptions for other posts on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to turn a post into a link to that post on stackoverflow.com\n",
    "def get_link(p, desc='has no UserId'):\n",
    "    # the link to a post is different for answers and questions\n",
    "    if p['PostType'] == 'Answer':\n",
    "        link = '\"https://stackoverflow.com/questions/{0}#answer-{1}\"'.format(int(p['QuestionId']), int(p['PostId']))\n",
    "        return '<a href='+link+' target=\"_blank\">Answer {0} {1}</a>'.format(int(p['PostId']), desc)\n",
    "    else:\n",
    "        link = '\"https://stackoverflow.com/questions/{0}\"'.format(int(p['PostId']))\n",
    "        return '<a href='+link+' target=\"_blank\">Question {0} {1}</a>'.format(int(p['PostId']), desc)\n",
    "\n",
    "# Take the first couple posts without user ids, turn them into links\n",
    "# join those links with the <br/> tag, and display the result as HTML\n",
    "display(HTML('<br/>'.join(posts[posts['UserId'].isnull()].head().apply(lambda p: get_link(p), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that unlike other user descriptions, the users of the linked posts had no link to their profile page and no information about their reputation. My theory is that these users deleted their accounts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"explore\"></div>\n",
    "## Text Visualization\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "How can we visualize text in the titles of each post?\n",
    "\n",
    "One method is to count up when a word is used and make statistical charts with [pandas](https://pandas.pydata.org) and [seaborn](https://seaborn.pydata.org/index.html).\n",
    "\n",
    "There are some words which appear a lot more than any others in any body of text. These are commonly referred to as \"stop words\" and they rarely give any information about a document when that document is modeled as a bag of words. For this reason, it is best to ignore these words when looking at the most common words. To do this we need a stop word list. Fortunately [Alireza Savand](https://github.com/Alir3z4) maintains lists of stop words in several languages on [Github](https://github.com/Alir3z4/stop-words). For this tutorial we will just read in the English list from Github, but if you want you can install his [library](https://github.com/Alir3z4/python-stop-words) for python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'python'\n",
    "python_titles = questions[questions['Tags'].str.contains('<'+tag+'>')]['Title'].str.lower()\n",
    "word_counter = Counter([word for phrase in python_titles.tolist() for word in re.split(\"[^\\w]+\", phrase)])\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
    "# url = '<path to local file>' # uncomment this line and add the path if you have downloaded the English stop words\n",
    "stop_words = pd.read_csv(url, header=None)[0].tolist()\n",
    "\n",
    "stop_word_count = 0\n",
    "words_to_use = 50\n",
    "\n",
    "# a list to collect words in the top 50 that are not stop words\n",
    "totals = []\n",
    "# a dataframe to occur whether or not a word appears in each post title\n",
    "occurs_in_post = pd.DataFrame()\n",
    "for word, count in word_counter.most_common(words_to_use):\n",
    "    # also filter out the empty string\n",
    "    if word in stop_words or len(word) == 0:\n",
    "        stop_word_count += 1\n",
    "        continue\n",
    "    totals.append((word, count))\n",
    "    occurs_in_post[word] = python_titles.apply(lambda s: 1 if word in re.split(\"[^\\w]+\", s) else 0)\n",
    "\n",
    "print('{0} of the top {1} words were stop words'.format(stop_word_count, words_to_use))\n",
    "\n",
    "# plot word counts\n",
    "totals = pd.DataFrame(totals, columns=['word', 'count'])\n",
    "totals.set_index('word', inplace=True)\n",
    "totals.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that users tend to write python in the title of their question in addition to tagging with the label python.\n",
    "\n",
    "Words do not occur independently of each other. In addition to counting how often each word appears we can also visualize correlations between when they co-appear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the correlation between word occurances\n",
    "corr = occurs_in_post.corr()\n",
    "\n",
    "# Generate a mask for the diagonal\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "diagonal_index = np.diag_indices(len(mask))\n",
    "mask[diagonal_index] = True\n",
    "\n",
    "# get the maximum value not on the diagonal\n",
    "corr_values = corr.values.copy()\n",
    "corr_values[diagonal_index] = 0\n",
    "corr_values = [val for row in corr_values for val in row]\n",
    "max_corr = max(corr_values)\n",
    "\n",
    "sns.clustermap(corr, mask=mask, cmap=\"BrBG\", vmax=max_corr, center=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that value, values, column, pandas and dataframe frequently co-occur. Django and Python usually do not appear in the same question title. Array and numpy often co-occur and can usually appears in titles with \"t\". Since we split on non alpha-numeric characters, this probably means that can and \"t\" were often two parts of \"can't\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Methods of Looking at Word Co-occurances\n",
    "\n",
    "While looking at correlations is better than just looking at word counts to understand words in context, it is still pretty far off from representing the full context in which we read. There are ways visualize context better. One of them is called a [Word Tree](http://hint.fm/papers/wordtree_final2.pdf) and was developed by the many eyes group at IBM. You can see [interactive examples](https://www.jasondavies.com/wordtree/) of this style of visualization made in d3.js.\n",
    "\n",
    "Since Jupyter notebooks can embed HTML elements, word trees rendered in d3 can be embedded in a notebook. However, for this tutorial we are sticking to python, and so far no one has implemented anything like [Bokeh](https://bokeh.pydata.org/en/latest/) for tree visualizations.\n",
    "\n",
    "The next cell demonstrates how to build a word tree using a library which runs [graphviz](http://graphviz.org) in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a variable to help us mark nodes as distinct when they have the same label\n",
    "node_counter = 0\n",
    "\n",
    "# a class to keep track of a node and it's connections\n",
    "class Node:\n",
    "    def __init__(self, word, count, matching_strings, graph, reverse=False, branching=3, highlight=False):\n",
    "        global node_counter\n",
    "        if highlight:\n",
    "            self.node = graphviz.Node(node_counter, label=word+'\\n'+str(count), peripheries=2, fontsize=20)\n",
    "        else:\n",
    "            self.node = graphviz.Node(node_counter, label=word+'\\n'+str(count))\n",
    "        node_counter += 1\n",
    "        graph.add_node(self.node)\n",
    "        if count > 1:\n",
    "            self.generate_children(matching_strings, graph, reverse, branching)\n",
    "    \n",
    "    def generate_children(self, matching_strings, graph, reverse, branching):\n",
    "        if len(matching_strings) == 0:\n",
    "            return\n",
    "        matching_strings = matching_strings[matching_strings.apply(len) > 0]\n",
    "        all_children = Counter(matching_strings.apply(lambda x:x[-1 if reverse else 0]))\n",
    "        children = all_children.most_common(branching)\n",
    "        for word, count in children:\n",
    "            if not reverse:\n",
    "                child_matches = matching_strings[matching_strings.apply(lambda x:x[0]) == word].apply(lambda x:x[1:])\n",
    "                c_node = Node(word, count, child_matches, graph=graph, reverse=reverse, branching=branching)\n",
    "                graph.add_edge(graphviz.Edge(self.node, c_node.node))\n",
    "            else:\n",
    "                child_matches = matching_strings[matching_strings.apply(lambda x:x[-1]) == word].apply(lambda x:x[:-1])\n",
    "                c_node = Node(word, count, child_matches, graph=graph, reverse=reverse, branching=branching)\n",
    "                graph.add_edge(graphviz.Edge(c_node.node, self.node))\n",
    "        left_over = sum(all_children.values()) - sum([x[1] for x in children])\n",
    "        if left_over > 0:\n",
    "            c_node = Node('...', left_over, [], graph=graph, reverse=reverse, branching=branching)\n",
    "            if reverse:\n",
    "                graph.add_edge(graphviz.Edge(c_node.node, self.node))\n",
    "            else:\n",
    "                graph.add_edge(graphviz.Edge(self.node, c_node.node))\n",
    "\n",
    "def build_tree(root_string, suffixes, prefixes):\n",
    "    graph = graphviz.Dot()\n",
    "    root = Node(root_string, len(suffixes), suffixes, graph, reverse=False, highlight=True)\n",
    "    root.generate_children(prefixes, graph, True, 3)\n",
    "    return Image(graph.create_png())\n",
    "\n",
    "def get_end(string, sub_string, reverse):\n",
    "    side = 0 if reverse else -1\n",
    "    return [x for x in re.split(r'[^\\w]+', string.lower().split(sub_string)[side]) if len(x) > 0]\n",
    "\n",
    "def select_text(phrase):\n",
    "    series = questions['Title']\n",
    "    instances = series[series.str.lower().str.contains(phrase)]\n",
    "    suffixes = instances.apply(lambda x: get_end(x, phrase, False))\n",
    "    prefixes = instances.apply(lambda x: get_end(x, phrase, True))\n",
    "    return build_tree(phrase, suffixes, prefixes)\n",
    "\n",
    "select_text('using python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a lot of people talk about doing something with files using python, but most of the 204 questions with the phrase 'using python' are not captured by the three most common previous or next words. Another thing this plot shows is that all six people who used the phrase 'using python 2' said 'using python 2.7'.\n",
    "\n",
    "To make this word tree, we looked at all question titles, not just the ones tagged 'python'. We can use our word tree function to explore how any phrase is used in question titles. For example, we can see if anyone mentioned 'september' in the first two weeks of September."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_text('september')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"time\"></div>\n",
    "## Visualizing Time\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "1. What time of day do people post?\n",
    "1. [How quickly were questions answered?](#firstReply)\n",
    "1. [Was the latency in answer time faster for some languages than others?](#langFirstReply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['CreationDate'].apply(lambda x: x.hour).hist(bins=range(24))\n",
    "plt.xlabel('Hour of day on a 24 hour clock')\n",
    "plt.ylabel('Number of posts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see anything odd in the distribution of when posts are created?\n",
    "\n",
    "<div id=\"firstReply\"></div>\n",
    "### How quickly were questions answered?\n",
    "1. When someone asked a question, how many minutes did it take to get an answer?\n",
    "1. Was there a difference in latency based on the language asked about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate answers by question id\n",
    "answers_by_question = answers.groupby('QuestionId')['CreationDate'].agg(min)\n",
    "# get the earliest creation date for each answer\n",
    "first_reply = pd.DataFrame({'PostId':answers_by_question.index.values, 'EarliestReply':answers_by_question.values})\n",
    "# add the time of the earliest answer to the questions data frame (filtering out questions which were not answered)\n",
    "first_reply = pd.merge(first_reply, questions, how='inner', on=['PostId'])\n",
    "\n",
    "# get the time it took to get an answer\n",
    "first_reply['Latency'] = (first_reply['EarliestReply']-first_reply['CreationDate'])\n",
    "# convert to minutes\n",
    "first_reply['Latency'] /= pd.Timedelta(minutes=1)\n",
    "\n",
    "# find the median\n",
    "print('Median answer time for questions asked and answered in the first two weeks of September 2017 is {0:.2f} min.'.format(first_reply['Latency'].median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea of the distribution of question to answer latency, let's plot a histogram of this data. Since this distribution probably has a long tail, let's use a log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the data\n",
    "first_reply['Latency'].hist(bins=50)\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.ylabel('Number of Questions')\n",
    "plt.xlabel('Time in Minutes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice a particularly odd value in this chart?\n",
    "\n",
    "How can we have an answer before the question was asked?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_questions = first_reply[first_reply['EarliestReply'] < first_reply['CreationDate']]\n",
    "links = weird_questions.apply(lambda p: get_link(p, desc='answered before question'), axis=1)\n",
    "display(HTML('<br/>'.join(links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"langFirstReply\"></div>\n",
    "### Was there a difference in latency based on the language asked about?\n",
    "\n",
    "We can also look at differences in latency between tags. For more complex numerical plots, [seaborn](https://seaborn.pydata.org/index.html) is really nice.\n",
    "\n",
    "As we saw in the histogram above, the distribution has a very long tail.\n",
    "For this reason I think it is easier to read without fliers (points beyond the whiskers) and on a log scale.\n",
    "You are welcome to set `showfliers` to `True` and comment out the line `ax.set(yscale=\"log\")` if you'd like to see the chart on a linear scale with fliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tags = ['javascript', 'python', 'java', 'r', 'c++', 'c']\n",
    "df = pd.DataFrame()\n",
    "for t in some_tags:\n",
    "    t_search = t.replace('+','\\+')\n",
    "    tag_items = pd.DataFrame(first_reply[first_reply['Tags'].str.contains('<'+t_search+'>')]['Latency'])\n",
    "    tag_items['Tag'] = t\n",
    "    df = df.append(tag_items)\n",
    "\n",
    "# uncomment the following line if you would like the chart sorted by the median latency\n",
    "# some_tags.sort(key = lambda t: df[df['Tag'] == t]['Latency'].median())\n",
    "\n",
    "ax = sns.boxplot(x=\"Tag\", y=\"Latency\", order=some_tags, data=df, showfliers=False)\n",
    "ax.set(yscale=\"log\")\n",
    "plt.ylabel('Latency in Minutes (Log Scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the median case it takes the longest to get an answer for an R question, but 75th percentile for latency is larger in Java than R.\n",
    "\n",
    "If you are currious about other languages, try editting `some_tags` in the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"network\"></div>\n",
    "## Plot Connections\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "We can model user interactions as a graph by making an edge from each person who answers a question to the person who posted that question. We can then use graphviz to visualize this graph.\n",
    "\n",
    "To start with, we need to compute the edges between non-null users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out null users and get question ids\n",
    "from_edges = answers.loc[answers['UserId'].notnull(),['UserId', 'QuestionId']]\n",
    "from_edges.rename(columns={'UserId':'AnswerUID', 'QuestionId':'PostId'}, inplace=True)\n",
    "# filter out null users and get question ids\n",
    "to_edges = questions.loc[questions['UserId'].notnull(),['UserId','PostId']]\n",
    "# merge on question id\n",
    "links = pd.merge(from_edges, to_edges, on='PostId', how='inner')\n",
    "# use a counter to merge duplicate edges to get edge weights\n",
    "edges = Counter(links.apply(lambda x:(x['AnswerUID'], x['UserId']), axis=1).tolist())\n",
    "edges.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use user reputation to color the nodes in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repuatation_map = dict(zip(posts['UserId'], posts['Reputation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some functions to visualize our graph.\n",
    "\n",
    "Graphviz has a few algorithms for deciding where nodes go. You can choose between them use the `prog` attribute when you create the graph image. The default option is dot, which works well for trees, but makes less sense for visualizing social networks. The full list of options is:\n",
    "* dot - \"hierarchical\" or layered drawings of directed graphs. This is the default tool to use if edges have directionality.\n",
    "* neato - \"spring model'' layouts.  This is the default tool to use if the graph is not too large (about 100 nodes) and you don't know anything else about it. Neato attempts to minimize a global energy function, which is equivalent to statistical multi-dimensional scaling.\n",
    "* fdp - \"spring model'' layouts similar to those of neato, but does this by reducing forces rather than working with energy.\n",
    "* twopi - radial layouts, after Graham Wills 97. Nodes are placed on concentric circles depending their distance from a given root node.\n",
    "* circo - circular layout, after Six and Tollis 99, Kauffman and Wiese 02. This is suitable for certain diagrams of multiple cyclic structures, such as certain telecommunications networks.\n",
    "\n",
    "In this example, nodes don't have labels and are filled according to a user's repuatation. If you would like to label nodes with the user's id, just change `label=''` to `label=uid` in `make_node`. In this example darker colors signify user's with less reputation. To change this just add `val = 255 - val` in `get_fill`. I like magenta, but if you want a different overall color to the graph, change the return statement for `get_fill`. These are some simple options:\n",
    "* gray = [val, val, val]\n",
    "* red = [val, 0, 0]\n",
    "* green = [0, val, 0]\n",
    "* blue = [0, 0, val]\n",
    "* yellow = [val, val, 0]\n",
    "* cyan = [0, val, val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to get a fill color based on reputation\n",
    "def get_fill(uid):\n",
    "    rep = repuatation_map[int(uid)]\n",
    "    # The distribution of reputations is very lop-sided, so let's use log reputation for our scale\n",
    "    # the value should be between 0 and 255\n",
    "    val = np.log(rep)/np.log(max(repuatation_map.values())) * 255\n",
    "    return to_hex([val, 0, val])\n",
    "\n",
    "# turn a RGB triplet into a hex color graphviz will understand\n",
    "def to_hex(triple):\n",
    "    output = '#'\n",
    "    for val in triple:\n",
    "        # the hex funciton returns a string of the form 0x<number in hex>\n",
    "        val = hex(int(val)).split('x')[1]\n",
    "        if len(val) < 2:\n",
    "            val = '0'+val\n",
    "        output += val\n",
    "    return output\n",
    "\n",
    "# The function to visualize our network graph\n",
    "# It takes in a list of edges with weights\n",
    "def build_network(edges_with_weights, prog='neato'):\n",
    "    # The function which builds each node. You can change the node style here.\n",
    "    make_node = lambda uid: graphviz.Node(uid, label='', shape='circle', style='filled', fillcolor=get_fill(uid), color='white')\n",
    "    graph = graphviz.Dot()\n",
    "    # A dictionary to keep track of node objects\n",
    "    nodes = {}\n",
    "    for pair in edges_with_weights:\n",
    "        e, w = pair\n",
    "        e = (str(int(e[0])), str(int(e[1])))\n",
    "        # Add notes to the graph if they don't exist yet\n",
    "        if e[0] not in nodes:\n",
    "            nodes[e[0]] = make_node(e[0])\n",
    "            graph.add_node(nodes[e[0]])\n",
    "        if e[1] not in nodes:\n",
    "            nodes[e[1]] = make_node(e[1])\n",
    "            graph.add_node(nodes[e[1]])\n",
    "        graph.add_edge(graphviz.Edge(nodes[e[0]], nodes[e[1]], penwidth=(float(w)/2)))\n",
    "    return Image(graph.create_png(prog=prog))\n",
    "\n",
    "# Let's build a small network from the edges with the highest weights.\n",
    "build_network(edges.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a two of the ten highest weighted edges are self loops. Who are the students who answer their own questions so many times and what are the questions where they post and answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in edges.most_common(10) if x[0][0] == x[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = 2700673\n",
    "self_links = links.loc[(links['AnswerUID'] == uid) & (links['UserId'] == uid),:].copy()\n",
    "self_links['PostType'] = 'Question'\n",
    "display(HTML('<br/>'.join(self_links.apply(lambda p: get_link(p, 'is a self link'), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the sample of edges above, this may not be a connected graph. Let's pick a sample of edges and plot a connected subgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_connected_subgraphs(edges):\n",
    "    nodes = list(set([n for e in edges for n in e]))\n",
    "    mappings = dict(zip(nodes, range(len(nodes))))\n",
    "    flipped_mappings = dict(zip(range(len(nodes)), [[n] for n in nodes]))\n",
    "    for e in edges:\n",
    "        c_1 = mappings[e[0]]\n",
    "        c_2 = mappings[e[1]]\n",
    "        if c_1 == c_2:\n",
    "            continue\n",
    "        if len(flipped_mappings[c_1]) > len(flipped_mappings[c_2]):\n",
    "            tmp = c_1\n",
    "            c_1 = c_2\n",
    "            c_2 = tmp\n",
    "        for n in flipped_mappings[c_1]:\n",
    "            mappings[n] = c_2\n",
    "            flipped_mappings[c_2].append(n)\n",
    "    return mappings\n",
    "\n",
    "num_edges = 2000\n",
    "connection_mapping = find_connected_subgraphs([x[0] for x in edges.most_common(num_edges)])\n",
    "Counter(connection_mapping.values()).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list above gives an identification number for a connected subgraph and the number of nodes which are in that subgraph.\n",
    "\n",
    "33 nodes seems like a large enough graph.\n",
    "\n",
    "*Note: If you plot a graph with over 500 nodes you might overwhelm graphviz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_id_number = 874\n",
    "e_list = [x for x in edges.most_common(num_edges) if connection_mapping.get(x[0][0],None) == subgraph_id_number]\n",
    "build_network(e_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that other than accounts which answer their own questions, no one in this graph is both a question asker and a question answerer.\n",
    "\n",
    "Is this because we are only looking at thick edges or can stack overflow users really be particianed into questioners and answerers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerers = set([e[0] for e in edges.keys() if e[0] != e[1]])\n",
    "questioners = set([e[1] for e in edges.keys() if e[0] != e[1]])\n",
    "both_question_and_answer = (answerers & questioners)\n",
    "\n",
    "print('{0:.2f}% of users both answered and asked questions'.format(len(both_question_and_answer)/len(answerers | questioners)))\n",
    "\n",
    "condition = lambda e: (e[0] in both_question_and_answer) or (e[1] in both_question_and_answer)\n",
    "connection_mapping = find_connected_subgraphs([e for e in edges if condition(e)])\n",
    "Counter(connection_mapping.values()).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_id_number = 6820\n",
    "e_list = [(e, edges[e]) for e in edges if condition(e) and connection_mapping.get(e[0],None) == subgraph_id_number]\n",
    "build_network(e_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that people who only answer have higher reputations than users who both post and answer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div id=\"place\"></div>\n",
    "## Plot Places\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "Where do people say they are from?\n",
    "\n",
    "In this example we'll look at how to make maps in python and why making maps is so hard.\n",
    "\n",
    "Let's start by adding information on each location from the [Google Maps API](https://developers.google.com/maps/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_data = pd.read_csv('loc_data.csv')\n",
    "location_data['Google_Data'] = location_data['Google_Data'].apply(lambda x: eval(x))\n",
    "\n",
    "location_data['Google_Data'].apply(len).hist()\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "plt.title('Was Google Able to Find a Unique Location from the Location String?')\n",
    "plt.xlabel('Number of Matches')\n",
    "plt.ylabel('Number of Unique Place Strings')\n",
    "plt.show()\n",
    "\n",
    "print(\"Let's only work with data that had a unique match\")\n",
    "location_data = location_data[location_data['Google_Data'].apply(len) == 1]\n",
    "posts_with_location = pd.merge(posts, location_data, how='inner', on='Location')\n",
    "\n",
    "def get_lowest_component(address_list):\n",
    "    components = address_list[0]['address_components']\n",
    "    all_parts = []\n",
    "    for c in components:\n",
    "        if len(c['types']) == 2 and c['types'][1] == 'political':\n",
    "            all_parts.append(c['types'][0])\n",
    "    if len(all_parts) > 0:\n",
    "        return all_parts[0]\n",
    "    return None\n",
    "\n",
    "posts_with_location = pd.merge(posts, location_data, how='inner', on='Location')\n",
    "lowest_components = posts_with_location['Google_Data'].apply(get_lowest_component).value_counts()\n",
    "lowest_components.plot(kind='bar', color='DarkBlue')\n",
    "plt.title(\"How precise is our location information?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add information on each level of location we have available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part(address_list, level='country', name_type='long_name'):\n",
    "    components = address_list[0]['address_components']\n",
    "    for c in components:\n",
    "        if c['types'] == [level, 'political']:\n",
    "            return c[name_type]\n",
    "    return None\n",
    "\n",
    "for part in lowest_components.index:\n",
    "    print(part)\n",
    "    posts_with_location[part] = posts_with_location['Google_Data'].apply(lambda x: get_part(x, level=part))\n",
    "\n",
    "posts_with_location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One way to plot location is to just make a convex hull around each set of latitude and longitude points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_long(address_list):\n",
    "    location = address_list[0]['geometry']['location']\n",
    "    return location['lat'], location['lng']\n",
    "\n",
    "posts_with_location['lat_lng'] = posts_with_location['Google_Data'].apply(get_lat_long)\n",
    "posts_with_location['lat'] = posts_with_location['lat_lng'].apply(lambda x: x[0])\n",
    "posts_with_location['lng'] = posts_with_location['lat_lng'].apply(lambda x: x[1])\n",
    "msno.geoplot(posts_with_location, x='lng', y='lat', by='country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a better map.\n",
    "\n",
    "In order to make a better map we need shape files which contain information on the boundries of each area we are interested in. We also need to link our data to these shape files which is usually far from trivial. In this example we will plot US counties because shape files on US states and counties are easy to find.\n",
    "\n",
    "I don't go over map projections here, but the projection of a map can be changed fairly easily. Shape files contain border information in latitude and longitude typically. You can write a projection function which can be applied to each cordinate before the map is plotted to capture the fact that the world is not square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_posts = posts_with_location.loc[posts_with_location['country'] == 'United States',:]\n",
    "county_posts = county_posts.groupby(['administrative_area_level_1', 'administrative_area_level_2'])\n",
    "\n",
    "def get_lang_count(series, lang='python'):\n",
    "    return len(series[series.notnull() & (series.str.find('<'+lang+'>') > -1)])\n",
    "\n",
    "county_stats = county_posts.agg({'lat':np.mean, 'lng':np.mean, 'PostId':len, 'Tags':get_lang_count,\n",
    "                                'PostType':lambda x : len(x[x=='Question'])})\n",
    "\n",
    "county_stats.reset_index(inplace=True)\n",
    "county_stats = county_stats.rename(columns={'PostId':'posts', 'Tags':'python questions', 'PostType':'questions', 'administrative_area_level_1':'State', 'administrative_area_level_2':'County'})\n",
    "county_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shape date for counties and states\n",
    "counties = {\n",
    "    code: county for code, county in counties.items()\n",
    "}\n",
    "\n",
    "us_states = us_states_data.data.copy()\n",
    "\n",
    "name_to_code = dict([(counties[code]['detailed name'], code) for code in counties])\n",
    "\n",
    "def match_county(county):\n",
    "    state = county['State']\n",
    "    county_name = county['County']\n",
    "    # take out non-ascii characters which are not in Bokeh file\n",
    "    county_name = unicodedata.normalize('NFKD', county_name).encode('ascii','ignore').decode(\"utf-8\")\n",
    "    full_name = county_name + ', ' + state\n",
    "    if full_name in name_to_code:\n",
    "        return name_to_code[full_name]\n",
    "    close_matches = [n for n in name_to_code.keys() if n.endswith(state) and n.startswith(county_name.split(' ')[0])]\n",
    "    if len(close_matches) == 0:\n",
    "        print(full_name)\n",
    "        return None\n",
    "    full_name = min(close_matches, key=len)\n",
    "    return name_to_code[full_name]\n",
    "\n",
    "county_stats['code'] = pd.Series(county_stats.apply(match_county, axis=1))\n",
    "county_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_map(county_stats, county_slice=None, language='python'):\n",
    "    color_mapper = LogColorMapper(palette=palette)\n",
    "    \n",
    "    if county_slice is not None:\n",
    "        county_stats = county_stats[county_slice]\n",
    "\n",
    "    county_xs = county_stats['code'].apply(lambda code: counties[code][\"lons\"]).tolist()\n",
    "    county_ys = county_stats['code'].apply(lambda code: counties[code][\"lats\"]).tolist()\n",
    "    county_names = (county_stats[\"County\"]+', '+county_stats[\"State\"]).tolist()\n",
    "    \n",
    "    language_perc = county_posts['Tags'].agg(lambda x: get_lang_count(x, language))\n",
    "    language_perc = language_perc.reset_index()\n",
    "    if county_slice is not None:\n",
    "        language_perc = language_perc[county_slice]\n",
    "    language_perc = (language_perc['Tags']/county_stats['questions'])\n",
    "    language_perc = (language_perc*100).tolist()\n",
    "\n",
    "    posts_source = ColumnDataSource(data=dict(\n",
    "        x=county_xs,\n",
    "        y=county_ys,\n",
    "        name=county_names,\n",
    "        posts=county_stats['posts'].tolist(),\n",
    "        questions=county_stats['questions'].tolist(),\n",
    "        lang_posts=language_perc\n",
    "    ))\n",
    "    \n",
    "    TOOLS = \"pan,wheel_zoom,reset,save\"\n",
    "\n",
    "    p = figure(\n",
    "        title=\"Posts by County\", tools=TOOLS,\n",
    "        x_axis_location=None, y_axis_location=None,\n",
    "        plot_width=900\n",
    "    )\n",
    "    p.grid.grid_line_color = None\n",
    "\n",
    "    county_pathches = Patches(xs=\"x\", ys=\"y\",\n",
    "              fill_color={'field': 'lang_posts', 'transform': color_mapper},\n",
    "              fill_alpha=0.7, line_color=\"white\", line_width=0.5)\n",
    "    county_pathches_render = p.add_glyph(posts_source, county_pathches)\n",
    "    \n",
    "    # add hover tooltip\n",
    "    hover = HoverTool(renderers=[county_pathches_render], tooltips=[\n",
    "        (\"Name\", \"@name\"),\n",
    "        (\"Posts\", \"@posts\"),\n",
    "        (\"Questions\", \"@questions\"),\n",
    "        (\"% \"+language.capitalize(), \"@lang_posts\")])\n",
    "    p.add_tools(hover)\n",
    "    \n",
    "    # -----------\n",
    "    # Add state outlines\n",
    "    # -----------\n",
    "    filter_fun = lambda x : x != 'AK' and x != 'HI'\n",
    "    # get lat and long as x and y\n",
    "    state_xs = [us_states[code][\"lons\"] for code in us_states if filter_fun(code)]\n",
    "    state_ys = [us_states[code][\"lats\"] for code in us_states if filter_fun(code)]\n",
    "    \n",
    "    # draw state lines\n",
    "    p.patches(state_xs, state_ys, fill_alpha=0.0, line_color=\"#\"+('9'*6), line_width=0.5)\n",
    "\n",
    "    show(p)\n",
    "\n",
    "build_map(county_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "build_map(county_stats, county_stats['questions'] > 7, language='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"conclusion\"></div>\n",
    "## Conclusion\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "Hopefully this tutorial tought you something new about visualization in python. Some key points I'd like to highlight are:\n",
    "\n",
    "1. visualize early - making visualizations early on can help a lot with data cleaning\n",
    "1. visualize for yourself first - visualization is a powerful tool for showing you what is going on in your data and if you can't understand your own visualizations, no one else will\n",
    "1. barcharts are awesome, but not everything should be a barchart - wordtrees, network diagrams and heatmaps are also nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
